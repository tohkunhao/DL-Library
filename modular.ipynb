{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modular.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOK0Uc793lVAAyWYUgG/2fv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tohkunhao/DL-Library/blob/main/modular.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1nHrepTlxWA"
      },
      "source": [
        "#import GPUtil\n",
        "import numpy as np\n",
        "import cupy as cp"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9QwhIYgp4xa"
      },
      "source": [
        "#def CheckGPU():\n",
        "#  try:\n",
        "#    GPUtil.getAvailable()\n",
        "#    status=\"available\"\n",
        "#  except:\n",
        "#    status=\"not available\"\n",
        "#  \n",
        "#  return status\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnP6E5dujCvs"
      },
      "source": [
        "class nnModule():\n",
        "  '''\n",
        "  class containing defaults of every NN module.\n",
        "  Default behaviour is that the module behaves the same when training and testing.\n",
        "  ニューラルネットワークのベースクラス。\n",
        "  デフォールトは学習とテスト時の処理が一緒。\n",
        "  '''\n",
        "  delta = 1e-07\n",
        "  def __init__(self):\n",
        "    self.params = {}\n",
        "    self.grads = {}\n",
        "    self.xp = None\n",
        "\n",
        "  def eval(self): #to be present in every NN module\n",
        "    pass\n",
        "  \n",
        "  def train(self): #to be present in every NN module\n",
        "    pass"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxhcIRTbqo40"
      },
      "source": [
        "class Linear(nnModule):\n",
        "  '''\n",
        "  takes in the arguments (in_channels, out_channels, init_type, bias)\n",
        "  in_channels is the number of input features\n",
        "  out_channels is the number of perceptrons\n",
        "  init_type is the type of weight initialisations. Default is He Kaiming's for use with ReLU\n",
        "    other options include Xavier for tanh.\n",
        "  bias determines if bias is used. Default is set to true.\n",
        "  updateflg will determine if weights will be updated by the optimizer. Default is set to true.\n",
        "  '''\n",
        "  def __init__(self, in_channels, out_channels, init_type='He',bias=True,updateflg=True):\n",
        "    super(Linear,self).__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.init_type = init_type\n",
        "    self.bias = bias\n",
        "    self.updateflg = updateflg\n",
        "  \n",
        "  def forward(self,x):\n",
        "    self.xp = cp.get_array_module(x)\n",
        "    self.x = x #store x for use in backprop\n",
        "\n",
        "    if self.init_type == 'He':\n",
        "      sd = self.xp.sqrt(2/self.in_channels)\n",
        "    elif self.init_type == 'Xavier':\n",
        "      sd = self.xp.sqrt(1/self.in_channels)\n",
        "\n",
        "    #initialise weights and biases\n",
        "    if 'w' not in self.params.keys():\n",
        "      self.params['w'] = self.xp.random.rand(self.in_channels,self.out_channels)*sd\n",
        "    if self.bias and 'b' not in self.params.keys():\n",
        "      self.params['b'] = self.xp.zeros(self.out_channels)#initialize at 0\n",
        "\n",
        "    if self.bias:\n",
        "      out = self.xp.dot(x,self.params['w'])+self.params['b']\n",
        "    else:\n",
        "      out = self.xp.dot(x,self.params['w'])\n",
        "    \n",
        "    return out\n",
        "\n",
        "  def backward(self,dout):\n",
        "    if self.bias:\n",
        "      self.grads['b'] = self.xp.sum(dout,axis=0)\n",
        "    \n",
        "    self.grads['w'] = self.xp.dot(self.x.reshape((self.in_channels,-1)),dout.reshape((-1,dout.shape[-1]))) #to prevent errors in batch size 1\n",
        "\n",
        "    return self.xp.dot(dout,self.xp.transpose(self.params['w']))\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvurszcLr4wQ"
      },
      "source": [
        "class ReLU(nnModule):\n",
        "  '''\n",
        "  Activation function ReLU\n",
        "  f(x)=x x>0, 0 x<0\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super(ReLU,self).__init__()\n",
        "    self.mask = None\n",
        "    self.updateflg = False #no trainable parameters\n",
        "  \n",
        "  def forward(self,x):\n",
        "    self.mask = x>0 #store for backprop\n",
        "    self.xp = cp.get_array_module(x)\n",
        "    return self.xp.maximum(0,x)\n",
        "  \n",
        "  def backward(self,dout):\n",
        "    out = self.xp.zeros_like(dout)\n",
        "    out[self.mask] = dout[self.mask]\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMPjTpUFRHXj"
      },
      "source": [
        "class Dropout(nnModule):\n",
        "  '''\n",
        "  Performs dropout on the outputs of the linear layer according to the\n",
        "  probability of p as defined by the user\n",
        "  '''\n",
        "  def __init__(self,p=0.5):\n",
        "    super(Dropout,self).__init__()\n",
        "    self.p = p\n",
        "    self.trainflg = True\n",
        "    self.mask = None\n",
        "    self.updateflg = False\n",
        "  \n",
        "  def forward(self,x):\n",
        "    if self.trainflg:\n",
        "      self.xp = cp.get_array_module(x)\n",
        "\n",
        "      rng = self.xp.random.rand(*x.shape)\n",
        "      out = self.xp.zeros_like(x)\n",
        "      self.mask = rng > self.p\n",
        "      out[self.mask] = x[self.mask]\n",
        "    else:\n",
        "      out = x * (1 - self.p) #scale the output at test time\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self,dout):\n",
        "    out = self.xp.zeros_like(dout)\n",
        "    out[self.mask] = dout[self.mask]\n",
        "    return out\n",
        "  \n",
        "  def eval(self):\n",
        "    self.trainflg = False #no dropout during test time\n",
        "  \n",
        "  def train(self):\n",
        "    self.trainflg = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wY-mMLZBQU4Q"
      },
      "source": [
        "class BatchNorm(nnModule):\n",
        "  '''\n",
        "  Performs batchnorm on NxD tensor from 2015 batchnorm paper by Ioffe & Szegedy.\n",
        "  NOT TO BE USED FOR CONVOLUTION. See BatchNormConv for convolutions.\n",
        "  N being mini batch size, D being number of features\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super(BatchNorm,self).__init__()\n",
        "    self.trainflg = True\n",
        "    self.updateflg = True\n",
        "    self.mean = [] \n",
        "    self.var = []\n",
        "    self.m = None\n",
        "    self.x_hat = None\n",
        "    self.mbmean = None\n",
        "    self.mbvar = None\n",
        "    self.x = None\n",
        "  \n",
        "  def forward(self,x):\n",
        "    self.xp = cp.get_array_module(x)\n",
        "    self.x = x #store for backprop\n",
        "\n",
        "    #initialise the trainable parameters\n",
        "    if not self.params.has_key('gamma'):\n",
        "      self.params['gamma'] = self.xp.ones(x.shape[-1]) #as many as input features\n",
        "    if not self.params.has_key('beta'):\n",
        "      self.params['beta'] = self.xp.zeros(x.shape[-1]) #as many as input features\n",
        "\n",
        "    if not self.m:\n",
        "      self.m = x.shape[0] #set minibatch size if self.m is None\n",
        "\n",
        "    if self.trainflg:\n",
        "      self.mbmean = self.xp.mean(x, axis = 0)\n",
        "      self.mbvar = self.xp.mean((x-self.mbmean)**2,axis = 0)\n",
        "      self.mean.append(self.mbmean) #used to calculate moving average\n",
        "      self.var.append(self.mbvar) #used to calculate moving average\n",
        "      self.x_hat = (x - self.mbmean)/self.xp.sqrt(self.mbvar + delta)\n",
        "    else:\n",
        "      Ex = self.xp.mean(self.mean,axis = 0)#moving average\n",
        "      Varx = (self.m/(self.m-1)) * self.xp.mean(self.var, axis = 0)#moving average\n",
        "      self.x_hat = (x - Ex)/self.xp.sqrt(Varx + delta)\n",
        "    \n",
        "    return self.params['gamma'] * self.x_hat + self.params['beta']\n",
        "    \n",
        "  def backward(self,dout):\n",
        "    #store trainable gradients\n",
        "    self.grads['beta'] = self.xp.sum(dout, axis = 0) #(D,) dimension\n",
        "    self.grads['gamma'] = self.xp.sum(dout*self.x_hat, axis = 0) #(D,) dimension\n",
        "\n",
        "    #Not trainable gradients. No need to store\n",
        "    dx_hat = self.params['gamma']*dout #(N,D) dimension\n",
        "    dmbvar = self.xp.sum(dx_hat*(self.x-self.mbmean)*-0.5*self.xp.sqrt(self.mbvar+delta)**-3,axis = 0) #(D,) dimension\n",
        "    dmbmean = self.xp.sum(dx_hat*-1/self.xp.sqrt(self.mbvar+delta),axis = 0)\n",
        "    dmbmean += self.mbvar*self.xp.sum(-2*(self.x-self.mbmean),axis = 0)/self.m #(D,) dimension\n",
        "    return dx_hat/self.xp.sqrt(self.mbvar+delta) + dmbvar*2*(self.x-self.mbmean)/self.m + dmbmean/self.m #(N,D) dimension\n",
        "  \n",
        "  def eval(self):\n",
        "    self.trainflg = False\n",
        "  \n",
        "  def train(self):\n",
        "    self.trainflg = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s94AXSKA7fXs"
      },
      "source": [
        "class Sequence(nnModule):\n",
        "  '''\n",
        "  class for executing nnModules in a sequential manner (left to right).\n",
        "  モジュールを順番に実行するクラス。\n",
        "  '''\n",
        "  def __init__(self,*layers):\n",
        "    super(Sequence,self).__init__()\n",
        "    self.modules = list(layers)\n",
        "    self.x = None\n",
        "\n",
        "  def forward(self,x):\n",
        "    self.x = x\n",
        "    for module in self.modules:\n",
        "      x = module.forward(x)\n",
        "    return x\n",
        "  \n",
        "  def backward(self,dout):\n",
        "    for module in self.modules[::-1]:\n",
        "      dout = module.backward(dout)\n",
        "    return dout\n",
        "\n",
        "  #need to add a way to update gradients of models contained in the sequence\n"
      ],
      "execution_count": 47,
      "outputs": []
    }
  ]
}