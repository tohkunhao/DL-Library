{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modular.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMsOCgVr6JGpJ/neZyaWfQ6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tohkunhao/DL-Library/blob/main/modular.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1nHrepTlxWA"
      },
      "source": [
        "#import GPUtil\n",
        "import numpy as np\n",
        "import cupy as cp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9QwhIYgp4xa"
      },
      "source": [
        "#def CheckGPU():\n",
        "#  try:\n",
        "#    GPUtil.getAvailable()\n",
        "#    status=\"available\"\n",
        "#  except:\n",
        "#    status=\"not available\"\n",
        "#  \n",
        "#  return status\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnP6E5dujCvs"
      },
      "source": [
        "class nnModule():\n",
        "  '''\n",
        "  class containing defaults of every NN module.\n",
        "  Use self.trainflg as a conditional if training and testing has different behaviour\n",
        "  学習時とテスト時の挙動が異なったら、self.trainflgという条件を使用してください\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    self.params = {}\n",
        "    self.grads = {}\n",
        "    self.trainflg = True\n",
        "    self.updateflg = True\n",
        "    self.xp = None\n",
        "\n",
        "  def eval(self): #to be present in every NN module\n",
        "    self.trainflg = False\n",
        "  \n",
        "  def train(self): #to be present in every NN module\n",
        "    self.trainflg = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxhcIRTbqo40"
      },
      "source": [
        "class Linear(nnModule):\n",
        "  '''\n",
        "  takes in the arguments (in_channels, out_channels, init_type, bias)\n",
        "  in_channels is the number of input features\n",
        "  out_channels is the number of perceptrons\n",
        "  init_type is the type of weight initialisations. Default is He Kaiming's for use with ReLU\n",
        "    other options include Xavier for tanh.\n",
        "  bias determines if bias is used. Default is set to true.\n",
        "  updateflg will determine if weights will be updated by the optimizer. Default is set to true.\n",
        "  '''\n",
        "  def __init__(self, in_channels, out_channels, init_type='He',bias=True,updateflg=True):\n",
        "    super(Linear,self).__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.init_type = init_type\n",
        "    self.bias = bias\n",
        "    self.updateflg = updateflg\n",
        "  \n",
        "  def forward(self,x):\n",
        "    self.xp = cp.get_array_module(x)\n",
        "    self.x = x #store x for use in backprop\n",
        "\n",
        "    if self.init_type == 'He':\n",
        "      sd = self.xp.sqrt(2/self.in_channels)\n",
        "    elif self.init_type == 'Xavier':\n",
        "      sd = self.xp.sqrt(1/self.in_channels)\n",
        "\n",
        "    #initialise weights and biases\n",
        "    if 'w' not in self.params.keys():\n",
        "      self.params['w'] = self.xp.random.rand(self.in_channels,self.out_channels)*sd\n",
        "    if self.bias and 'b' not in self.params.keys():\n",
        "      self.params['b'] = self.xp.zeros(self.out_channels)#initialize at 0\n",
        "\n",
        "    if self.bias:\n",
        "      out = self.xp.dot(x,self.params['w'])+self.params['b']\n",
        "    else:\n",
        "      out = self.xp.dot(x,self.params['w'])\n",
        "    \n",
        "    return out\n",
        "\n",
        "  def backward(self,dout):\n",
        "    if self.bias:\n",
        "      self.grads['b'] = self.xp.sum(dout,axis=0)\n",
        "    \n",
        "    self.grads['w'] = self.xp.dot(self.x.reshape((self.in_channels,-1)),dout.reshape((-1,dout.shape[-1]))) #to prevent errors in batch size 1\n",
        "\n",
        "    return self.xp.dot(dout,self.xp.transpose(self.params['w']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvurszcLr4wQ"
      },
      "source": [
        "class ReLU(nnModule):\n",
        "  '''\n",
        "  Activation function ReLU\n",
        "  f(x)=x x>0, 0 x<0\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super(ReLU,self).__init__()\n",
        "    self.mask = None\n",
        "    self.updateflg = False #no trainable parameters\n",
        "  \n",
        "  def forward(self,x):\n",
        "    self.mask = x>0 #store for backprop\n",
        "    self.xp = cp.get_array_module(x)\n",
        "    return self.xp.maximum(0,x)\n",
        "  \n",
        "  def backward(self,dout):\n",
        "    out = self.xp.zeros_like(dout)\n",
        "    out[self.mask] = dout[self.mask]\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMPjTpUFRHXj"
      },
      "source": [
        "class Dropout(nnModule):\n",
        "  '''\n",
        "  Performs dropout on the outputs of the linear layer according to the\n",
        "  probability of p as defined by the user\n",
        "  '''\n",
        "  def __init__(self,p=0.5):\n",
        "    super(Dropout,self).__init__()\n",
        "    self.p = p\n",
        "    self.mask = None\n",
        "    self.updateflg = False\n",
        "  \n",
        "  def forward(self,x):\n",
        "    if self.trainflg:\n",
        "      self.xp = cp.get_array_module(x)\n",
        "\n",
        "      rng = self.xp.random.rand(*x.shape)\n",
        "      out = self.xp.zeros_like(x)\n",
        "      self.mask = rng > self.p\n",
        "      out[self.mask] = x[self.mask]\n",
        "    else:\n",
        "      out = x * (1 - self.p) #scale the output at test time\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self,dout):\n",
        "    out = self.xp.zeros_like(dout)\n",
        "    out[self.mask] = dout[self.mask]\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wY-mMLZBQU4Q"
      },
      "source": [
        "class BatchNorm(nnModule):\n",
        "  '''\n",
        "  Performs batchnorm on NxD tensor from 2015 batchnorm paper by Ioffe & Szegedy.\n",
        "  NOT TO BE USED FOR CONVOLUTION. See BatchNormConv for convolutions.\n",
        "  N being mini batch size, D being number of features\n",
        "  '''\n",
        "  def __init__(self,eps=1e-05):\n",
        "    super(BatchNorm,self).__init__()\n",
        "    self.mean = [] \n",
        "    self.var = []\n",
        "    self.m = None\n",
        "    self.x_hat = None\n",
        "    self.mbmean = None\n",
        "    self.mbvar = None\n",
        "    self.x = None\n",
        "    self.eps = eps\n",
        "  \n",
        "  def forward(self,x):\n",
        "    self.xp = cp.get_array_module(x)\n",
        "    self.x = x #store for backprop\n",
        "\n",
        "    #initialise the trainable parameters\n",
        "    if 'gamma' not in self.params.keys():\n",
        "      self.params['gamma'] = self.xp.ones(x.shape[-1]) #as many as input features\n",
        "    if 'beta' not in self.params.keys():\n",
        "      self.params['beta'] = self.xp.zeros(x.shape[-1]) #as many as input features\n",
        "\n",
        "    if not self.m:\n",
        "      self.m = x.shape[0] #set minibatch size if self.m is None\n",
        "\n",
        "    if self.trainflg:\n",
        "      self.mbmean = self.xp.mean(x, axis = 0)\n",
        "      self.mbvar = self.xp.mean((x-self.mbmean)**2,axis = 0)\n",
        "      self.mean.append(self.mbmean) #used to calculate moving average\n",
        "      self.var.append(self.mbvar) #used to calculate moving average\n",
        "      self.x_hat = (x - self.mbmean)/self.xp.sqrt(self.mbvar + self.eps)\n",
        "    else:\n",
        "      Ex = self.xp.mean(self.mean,axis = 0)#moving average\n",
        "      Varx = (self.m/(self.m-1)) * self.xp.mean(self.var, axis = 0)#moving average\n",
        "      self.x_hat = (x - Ex)/self.xp.sqrt(Varx + self.eps)\n",
        "    \n",
        "    return self.params['gamma'] * self.x_hat + self.params['beta']\n",
        "    \n",
        "  def backward(self,dout):\n",
        "    #store trainable gradients\n",
        "    self.grads['beta'] = self.xp.sum(dout, axis = 0) #(D,) dimension\n",
        "    self.grads['gamma'] = self.xp.sum(dout*self.x_hat, axis = 0) #(D,) dimension\n",
        "\n",
        "    #Not trainable gradients. No need to store\n",
        "    dx_hat = self.params['gamma']*dout #(N,D) dimension\n",
        "    dmbvar = self.xp.sum(dx_hat*(self.x-self.mbmean)*-0.5*self.xp.sqrt(self.mbvar+self.eps)**-3,axis = 0) #(D,) dimension\n",
        "    dmbmean = self.xp.sum(dx_hat*-1/self.xp.sqrt(self.mbvar+self.eps),axis = 0)\n",
        "    dmbmean += self.mbvar*self.xp.sum(-2*(self.x-self.mbmean),axis = 0)/self.m #(D,) dimension\n",
        "    return dx_hat/self.xp.sqrt(self.mbvar+self.eps) + dmbvar*2*(self.x-self.mbmean)/self.m + dmbmean/self.m #(N,D) dimension"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s94AXSKA7fXs"
      },
      "source": [
        "class Sequence(nnModule):\n",
        "  '''\n",
        "  class for executing nnModules in a sequential manner (left to right).\n",
        "  non nested lists are accepted.\n",
        "  モジュールを順番に実行するクラス。\n",
        "  '''\n",
        "  def __init__(self,*layers):\n",
        "    super(Sequence,self).__init__()\n",
        "    self.modules = []\n",
        "    for layer in layers:\n",
        "      if type(layer) == list:\n",
        "        for sublayer in layer:\n",
        "          self.modules.append(sublayer)\n",
        "      else:\n",
        "        self.modules.append(layer)\n",
        "    self.x = None\n",
        "\n",
        "  def forward(self,x):\n",
        "    self.x = x\n",
        "    for module in self.modules:\n",
        "      x = module.forward(x)\n",
        "    return x\n",
        "  \n",
        "  def backward(self,dout):\n",
        "    for module in self.modules[::-1]:\n",
        "      dout = module.backward(dout)\n",
        "    return dout\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2ccEik5Q74_"
      },
      "source": [
        "def im2col(image, kernel_size, stride, padding,mode='reg'):\n",
        "  '''畳み込みニューラルネットワーク用のim2col関数。\n",
        "  col_out = im2col(image, kernel_size, stride, padding)\n",
        "  3 modes available, reg, mono, single\n",
        "  image format for the 3 modes:\n",
        "  reg: N,C,H,W\n",
        "  mono: N,H,W\n",
        "  single: C,H,W\n",
        "  kernel_size, stride and padding can be a tuple\n",
        "  '''\n",
        "\n",
        "  xp = cp.get_array_module(image)\n",
        "\n",
        "  if mode == 'reg':\n",
        "    N,C,H,W = image.shape\n",
        "  else:\n",
        "    N,C = 1, 1\n",
        "    unk,H,W = image.shape\n",
        "    if mode == 'mono':\n",
        "      N = unk\n",
        "    elif mode == 'single':\n",
        "      C = unk\n",
        "    image = image.reshape(N,C,H,W) #make into 4d array\n",
        "\n",
        "  kh, kw = tupleoption(kernel_size)\n",
        "  ph, pw = tupleoption(padding)\n",
        "  sh, sw = tupleoption(stride)\n",
        "\n",
        "  #add padding\n",
        "  image = xp.pad(image,[(0,0),(0,0),(ph,ph),(pw,pw)],'constant')\n",
        "\n",
        "  out_h = int((H+2*ph-kh)/sh +1)\n",
        "  out_w = int((W+2*pw-kw)/sw +1)\n",
        "  #print(out_h)\n",
        "  #print(out_w)\n",
        "\n",
        "  col_holder = []\n",
        "\n",
        "  for batch_sample in range(N):\n",
        "    for h_move in range(0,H-kh+1,sh):\n",
        "      h_start = 0+h_move\n",
        "      h_end = h_start+kh\n",
        "      for w_move in range(0,W-kw+1,sw):\n",
        "        w_start = 0+w_move\n",
        "        w_end = w_start+kw\n",
        "        col_holder.append(xp.reshape(image[batch_sample][:,h_start:h_end,w_start:w_end],-1))\n",
        "  \n",
        "  return xp.array(col_holder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5_ky-OnUq8E",
        "outputId": "535a67cd-2a3d-4c9c-c81c-e7aa6a6bf897"
      },
      "source": [
        "def tupleoption(input_var):\n",
        "  if isinstance(input_var, int):\n",
        "    return input_var, input_var\n",
        "  else: \n",
        "    return input_var[0], input_var[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 2]\n"
          ]
        }
      ]
    }
  ]
}